{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f63f67f2-e46c-4c80-b18f-b42474a51171",
   "metadata": {},
   "source": [
    "# Webcrawler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e8187-4804-49b3-be97-50597c66c2ee",
   "metadata": {},
   "source": [
    "### Importierte Bibliotheken\n",
    "\n",
    "Diese Bibliotheken werden verwendet, um HTTP-Anfragen zu senden, HTML-Inhalte zu analysieren, URL-Manipulationen durchzuführen und Threading zu verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc244d1-a9fd-4d09-8ef9-e0ea0ed6fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  \n",
    "from urllib.parse import urljoin, urlparse \n",
    "from urllib.robotparser import RobotFileParser  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f203a2-05b3-4463-9db1-e443663720cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import requests  \n",
    "import threading  \n",
    "import time  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcc5fa0-703e-4701-baaa-c6a5c0bcdec1",
   "metadata": {},
   "source": [
    "Erklärung:\n",
    "- BeautifulSoup: Werkzeug zum Parsen und Analysieren von HTML- und XML-Dokumenten.\n",
    "- urljoin, urlparse: Funktionen zur Bearbeitung und Manipulation von URLs.\n",
    "- RobotFileParser: Klasse zur Analyse und Interpretation der robots.txt-Datei, um die Crawling-Regeln einer Website zu überprüfen.\n",
    "- os: Modul zur Interaktion mit dem Betriebssystem (z. B. für Dateioperationen).\n",
    "- requests: Bibliothek zum Senden von HTTP-Anfragen an Webseiten.\n",
    "- threading: Modul zum Erstellen und Verwalten von Threads für gleichzeitige Operationen.\n",
    "- time: Modul zur Implementierung von Verzögerungen (z. B. zwischen Anfragen)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae0ff92-4cd4-491b-8321-72d668e3738e",
   "metadata": {},
   "source": [
    "### Funktion, um zu prüfen, ob das Crawlen einer URL erlaubt ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26826c2d-951e-40d8-a0d4-be0b425b4d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_fetch(url, user_agent='*'):\n",
    "    parsed_url = urlparse(url) \n",
    "    base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\" \n",
    "    rp = RobotFileParser()  \n",
    "    rp.set_url(urljoin(base_url, '/robots.txt')) \n",
    "    rp.read() \n",
    "    return rp.can_fetch(user_agent, url)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e8ae9-0cb1-4ec0-8703-1aef7bdb9b2c",
   "metadata": {},
   "source": [
    "Die Funktion überprüft die `robots.txt`-Datei der Website, um zu sehen, ob der angegebene User-Agent auf die URL zugreifen darf.\n",
    "Erklärung:\n",
    "- can_fetch: Funktion zur Überprüfung, ob ein User-Agent eine bestimmte URL gemäß robots.txt crawlen darf.\n",
    "- urlparse: Teilt die gegebene URL in ihre Komponenten (Schema, Host, Pfad usw.).\n",
    "- base_url: Erzeugt die Basis-URL, um die robots.txt-Datei zu erreichen.\n",
    "- RobotFileParser: Ein Objekt, das die Regeln aus robots.txt einliest und anwendet.\n",
    "- can_fetch: Überprüft, ob die gegebene URL für den angegebenen User-Agent zugänglich ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4325b0f7-a286-4dc0-a4cf-e9f7b33d3c2f",
   "metadata": {},
   "source": [
    "### Hauptfunktion zum Crawlen von Webseiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b92794-2a4d-4a99-8a81-4aed515d5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(url, max_depth=2, delay=1):\n",
    "    visited = set()  \n",
    "    lock = threading.Lock()  \n",
    "\n",
    "  \n",
    "    def _crawl(current_url, depth):\n",
    "        \n",
    "        if depth > max_depth or current_url in visited:\n",
    "            return\n",
    "\n",
    "        \n",
    "        with lock:\n",
    "            print(f\"Crawling: {current_url} at depth {depth}\")\n",
    "            visited.add(current_url)  \n",
    "\n",
    "        \n",
    "        if not can_fetch(current_url):\n",
    "            print(f\"Access denied by robots.txt for {current_url}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            \n",
    "            response = requests.get(current_url, timeout=10)\n",
    "            response.raise_for_status() \n",
    "        except requests.RequestException as e:\n",
    "            \n",
    "            print(f\"Failed to retrieve {current_url}: {e}\")\n",
    "            return\n",
    "\n",
    "       \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        \n",
    "        with lock:\n",
    "            with open(\"crawled_links.txt\", \"a\") as file:\n",
    "                file.write(current_url + \"\\n\")  \n",
    "\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link.get('href')  \n",
    "            full_url = urljoin(current_url, href)  \n",
    "            if full_url not in visited:\n",
    "                time.sleep(delay)  \n",
    "                _crawl(full_url, depth + 1)  \n",
    "\n",
    "    \n",
    "    threads = []\n",
    "    for i in range(5):  \n",
    "        t = threading.Thread(target=_crawl, args=(url, 0))\n",
    "        t.start() \n",
    "        threads.append(t)\n",
    "\n",
    "    \n",
    "    for t in threads:\n",
    "        t.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb96ddf1-2e9a-4aa3-a019-f7036afcf834",
   "metadata": {},
   "source": [
    "Erklärung:\n",
    "- crawl: Hauptfunktion zum Starten des Web-Crawling-Prozesses.\n",
    "- visited: Set zur Speicherung bereits besuchter URLs, um Doppelbesuche zu vermeiden.\n",
    "- lock: Ein Mechanismus zur Gewährleistung der Thread-Sicherheit beim Zugriff auf geteilte Ressourcen.\n",
    "- _crawl: Rekursive Hilfsfunktion zum Crawlen der gegebenen URL bis zur maximalen Tiefe.\n",
    "- requests.get: Senden einer HTTP-Anfrage an die aktuelle URL.\n",
    "- BeautifulSoup: Parsen der HTML-Inhalte zur Extraktion von Links.\n",
    "- soup.find_all('a', href=True): Sucht alle Anker-Elemente mit einem href-Attribut.\n",
    "- time.sleep(delay): Fügt eine Verzögerung zwischen den Anfragen hinzu, um die Serverbelastung zu reduzieren.\n",
    "- threading.Thread: Erzeugt und startet Threads für paralleles Crawlen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d8b800-5b03-4fce-be59-73f8e7c20c8e",
   "metadata": {},
   "source": [
    "### Benutzerinteraktion und Skriptausführung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68afaf27-e13a-48c0-85de-1c98fca51005",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url = input(\"Bitte geben Sie die Start-URL ein: \")\n",
    "\n",
    "if not start_url.startswith(('http://', 'https://')):\n",
    "    start_url = 'https://' + start_url\n",
    "\n",
    "max_depth = int(input(\"Bitte geben Sie die maximale Tiefe ein: \"))\n",
    "delay = float(input(\"Bitte geben Sie die Verzögerung zwischen den Anfragen in Sekunden ein: \"))\n",
    "\n",
    "if os.path.exists(\"crawled_links.txt\"):\n",
    "    os.remove(\"crawled_links.txt\")\n",
    "\n",
    "crawl(start_url, max_depth, delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80500bf",
   "metadata": {},
   "source": [
    "Erklärung:\n",
    "- input: Fordert den Benutzer zur Eingabe der Start-URL, der maximalen Tiefe und der Verzögerung auf.\n",
    "- startswith: Überprüft, ob die URL mit http:// oder https:// beginnt, und fügt andernfalls https:// hinzu.\n",
    "- os.path.exists: Überprüft, ob die Datei crawled_links.txt existiert, und löscht sie, wenn sie existiert.\n",
    "- crawl: Startet den Crawl-Prozess mit den angegebenen Parametern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
